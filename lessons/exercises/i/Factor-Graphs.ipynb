{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Factor Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Preliminaries\n",
    "\n",
    "- Goal \n",
    "  - Introduction to Forney-style factor graphs and message passing-based inference \n",
    "- Materials        \n",
    "  - Mandatory\n",
    "    - These lecture notes   \n",
    "    - Loeliger (2007), [The factor graph approach to model based signal processing](https://github.com/bertdv/BMLIP/blob/master/lessons/notebooks/files/Loeliger-2007-The-factor-graph-approach-to-model-based-signal-processing.pdf), pp. 1295-1302 (until section V)\n",
    "  - Optional\n",
    "    - Frederico Wadehn (2015), [Probabilistic graphical models: Factor graphs and more](https://www.youtube.com/watch?v=Fv2YbVg9Frc&t=31) video lecture (**highly recommended**)\n",
    "  - References\n",
    "    - Forney (2001), [Codes on graphs: normal realizations](https://github.com/bertdv/BMLIP/blob/master/lessons/notebooks/files/Forney-2001-Codes-on-graphs-normal-realizations.pdf)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why Factor Graphs?\n",
    "\n",
    "- A probabilistic inference task gets its computational load mainly through the need for marginalization (i.e., computing integrals). E.g., for a model $p(x_1,x_2,x_3,x_4,x_5)$, the inference task $p(x_2|x_3)$ is given by \n",
    "$$\n",
    "p(x_2|x_3) = \\frac{p(x_2,x_3)}{p(x_3)} = \\frac{\\int \\cdots \\int p(x_1,x_2,x_3,x_4,x_5) \\, \\mathrm{d}x_1  \\mathrm{d}x_4 \\mathrm{d}x_5}{\\int \\cdots \\int p(x_1,x_2,x_3,x_4,x_5) \\, \\mathrm{d}x_1  \\mathrm{d}x_2 \\mathrm{d}x_4 \\mathrm{d}x_5}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Since these computations (integrals or sums) suffer from the \"curse of dimensionality\", we often need to solve a simpler problem in order to get an answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Factor graphs provide a computationally efficient approach to solving inference problems **if the probabilistic model can be factorized**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- <a id='factorization-helps'>Factorization helps</a>. For instance, if $p(x_1,x_2,x_3,x_4,x_5) = p(x_1)p(x_2,x_3)p(x_4)p(x_5|x_4)$, then\n",
    "\n",
    "$$\n",
    "p(x_2|x_3) = \\frac{\\int \\cdots \\int p(x_1)p(x_2,x_3)p(x_4)p(x_5|x_4) \\, \\mathrm{d}x_1  \\mathrm{d}x_4 \\mathrm{d}x_5}{\\int \\cdots \\int p(x_1)p(x_2,x_3)p(x_4)p(x_5|x_4) \\, \\mathrm{d}x_1  \\mathrm{d}x_2 \\mathrm{d}x_4 \\mathrm{d}x_5} \n",
    "  = \\frac{p(x_2,x_3)}{\\int p(x_2,x_3) \\mathrm{d}x_2}\n",
    "$$\n",
    "\n",
    "which is computationally much cheaper than the general case above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In this lesson, we discuss how computationally efficient inference in *factorized* probability distributions can be automated by message passing-based inference in factor graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Factor Graph Construction Rules\n",
    "\n",
    "- Consider a function \n",
    "$$\n",
    "f(x_1,x_2,x_3,x_4,x_5) = f_a(x_1,x_2,x_3) \\cdot f_b(x_3,x_4,x_5) \\cdot f_c(x_4)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The factorization of this function can be graphically represented by a **Forney-style Factor Graph** (FFG):\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"./figures/ffg-example-1.png\" width=\"400px\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- An FFG is an **undirected** graph subject to the following construction rules ([Forney, 2001](https://github.com/bertdv/BMLIP/blob/master/lessons/notebooks/files/Forney-2001-Codes-on-graphs-normal-realizations.pdf))\n",
    "\n",
    "  1. A **node** for every factor;\n",
    "  1. An **edge** (or **half-edge**) for every variable;\n",
    "  1. Node $f_\\bullet$ is connected to edge $x$ **iff** variable $x$ appears in factor $f_\\bullet$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A **configuration** is an assigment of values to all variables.\n",
    "\n",
    "- A configuration $\\omega=(x_1,x_2,x_3,x_4,x_5)$ is said to be **valid** iff $f(\\omega) \\neq 0$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Equality Nodes for Branching Points\n",
    "\n",
    "\n",
    "- Note that a variable can appear in maximally two factors in an FFG (since an edge has only two end points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Consider the factorization (where $x_2$ appears in three factors) \n",
    "\n",
    "$$\n",
    " f(x_1,x_2,x_3,x_4) = f_a(x_1,x_2)\\cdot f_b(x_2,x_3) \\cdot f_c(x_2,x_4)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For the factor graph representation, we will instead consider the function $g$, defined as\n",
    "$$\\begin{align*}\n",
    " g(x_1,x_2&,x_2^\\prime,x_2^{\\prime\\prime},x_3,x_4) \n",
    "  = f_a(x_1,x_2)\\cdot f_b(x_2^\\prime,x_3) \\cdot f_c(x_2^{\\prime\\prime},x_4) \\cdot f_=(x_2,x_2^\\prime,x_2^{\\prime\\prime})\n",
    "\\end{align*}$$\n",
    "  where \n",
    "$$\n",
    "f_=(x_2,x_2^\\prime,x_2^{\\prime\\prime}) \\triangleq \\delta(x_2-x_2^\\prime)\\, \\delta(x_2-x_2^{\\prime\\prime})\n",
    "$$\n",
    "\n",
    " <p style=\"text-align:center;\"><img src=\"./figures/ffg-wEquality-node.png\" width=\"450px\"></p>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Note that through introduction of auxiliary variables $X_2^{\\prime}$ and $X_2^{\\prime\\prime}$ and a factor $f_=(x_2,x_2^\\prime,x_2^{\\prime\\prime})$, each variable in $g$ appears in maximally two factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The constraint $f_=(x,x^\\prime,x^{\\prime\\prime})$ enforces that $X=X^\\prime=X^{\\prime\\prime}$ **for every valid configuration**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Since $f$ is a marginal of $g$, i.e., \n",
    "$$\n",
    "f(x_1,x_2,x_3,x_4) = \\iint g(x_1,x_2,x_2^\\prime,x_2^{\\prime\\prime},x_3,x_4)\\, \\mathrm{d}x_2^\\prime \\mathrm{d}x_2^{\\prime\\prime}\n",
    "$$\n",
    "it follows that any inference problem on $f$ can be executed by a corresponding inference problem on $g$, e.g.,\n",
    "$$\\begin{align*}\n",
    "f(x_1 \\mid x_2) &\\triangleq \\frac{\\iint f(x_1,x_2,x_3,x_4) \\,\\mathrm{d}x_3 \\mathrm{d}x_4 }{ \\int\\cdots\\int f(x_1,x_2,x_3,x_4) \\,\\mathrm{d}x_1 \\mathrm{d}x_3 \\mathrm{d}x_4} \\\\\n",
    "  &= \\frac{\\int\\cdots\\int g(x_1,x_2,x_2^\\prime,x_2^{\\prime\\prime},x_3,x_4) \\,\\mathrm{d}x_2^\\prime \\mathrm{d}x_2^{\\prime\\prime} \\mathrm{d}x_3 \\mathrm{d}x_4 }{ \\int\\cdots\\int g(x_1,x_2,x_2^\\prime,x_2^{\\prime\\prime},x_3,x_4) \\,\\mathrm{d}x_1 \\mathrm{d}x_2^\\prime \\mathrm{d}x_2^{\\prime\\prime} \\mathrm{d}x_3 \\mathrm{d}x_4} \\\\\n",
    "  &= g(x_1 \\mid x_2)\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\Rightarrow$ **Any factorization of a global function $f$ can be represented by a Forney-style Factor Graph**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Probabilistic Models as Factor Graphs\n",
    "\n",
    "- FFGs can be used to express conditional independence (factorization) in probabilistic models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For example, the (previously shown) graph for \n",
    "$f_a(x_1,x_2,x_3) \\cdot f_b(x_3,x_4,x_5) \\cdot f_c(x_4)$ \n",
    "could represent the probabilistic model\n",
    "$$\n",
    "p(x_1,x_2,x_3,x_4,x_5) = p(x_1,x_2|x_3) \\cdot p(x_3,x_5|x_4) \\cdot p(x_4)\n",
    "$$\n",
    "where we identify \n",
    "$$\\begin{align*}\n",
    "f_a(x_1,x_2,x_3) &= p(x_1,x_2|x_3) \\\\\n",
    "f_b(x_3,x_4,x_5) &= p(x_3,x_5|x_4) \\\\\n",
    "f_c(x_4) &= p(x_4)\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This is the graph\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"./figures/ffg-example-prob-model.png\" width=\"400px\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inference by Closing Boxes\n",
    "\n",
    "- Factorizations provide opportunities to cut on the amount of needed computations when doing inference. In what follows, we will use FFGs to process these opportunities in an automatic way by message passing between the nodes of the graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Assume we wish to compute the marginal\n",
    "$$\n",
    "\\bar{f}(x_3) \\triangleq \\sum_{x_1,x_2,x_4,x_5,x_6,x_7}f(x_1,x_2,\\ldots,x_7) \n",
    "$$\n",
    "for a model $f$ with given factorization \n",
    "$$\n",
    "f(x_1,x_2,\\ldots,x_7) = f_a(x_1) f_b(x_2) f_c(x_1,x_2,x_3) f_d(x_4) f_e(x_3,x_4,x_5) f_f(x_5,x_6,x_7) f_g(x_7)\n",
    "$$ \n",
    "- The FFG for $f$ is (we will discuss the usage of directed edges below):\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"./figures/ffg-message-passing-1.png\" width=\"600px\"><p style=\"text-align:center;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Due to the factorization and the [Generalized Distributive Law](https://en.wikipedia.org/wiki/Generalized_distributive_law), we can decompose this sum-of-products to the following product-of-sums:\n",
    "$$\\begin{align*}\\bar{f}&(x_3) = \\\\\n",
    "  &\\underbrace{ \\Bigg( \\sum_{x_1,x_2} \\underbrace{f_a(x_1)}_{\\overrightarrow{\\mu}_{X_1}(x_1)}\\, \\underbrace{f_b(x_2)}_{\\overrightarrow{\\mu}_{X_2}(x_2)}\\,f_c(x_1,x_2,x_3)\\Bigg) }_{\\overrightarrow{\\mu}_{X_3}(x_3)} \n",
    "  \\underbrace{ \\cdot\\Bigg( \\sum_{x_4,x_5} \\underbrace{f_d(x_4)}_{\\overrightarrow{\\mu}_{X_4}(x_4)}\\,f_e(x_3,x_4,x_5) \\cdot \\underbrace{ \\big( \\sum_{x_6,x_7} f_f(x_5,x_6,x_7)\\,\\underbrace{f_g(x_7)}_{\\overleftarrow{\\mu}_{X_7}(x_7)}\\big) }_{\\overleftarrow{\\mu}_{X_5}(x_5)} \\Bigg) }_{\\overleftarrow{\\mu}_{X_3}(x_3)}\n",
    "\\end{align*}$$\n",
    "which is computationally (much) lighter than executing the full sum $\\sum_{x_1,\\ldots,x_7}f(x_1,x_2,\\ldots,x_7)$\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"./figures/ffg-message-passing.png\" width=\"700px\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that the new factor $\\overrightarrow{\\mu}_{X_3}(x_3)$ is obtained by multiplying all enclosed factors ($f_a$, $f_b, f_c$) by the red dashed box, followed by marginalization (summing) over all enclosed variables ($x_1$, $x_2$).\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This is the **Closing the Box**-rule, which is a general recipe for marginalization of latent variables (inside the box) and leads to a new factor that has the variables (edges) that cross the box as arguments. For instance, the argument of the remaining factor $\\overrightarrow{\\mu}_{X_3}(x_3)$ is the variable on the edge that crosses the red box ($x_3$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hence, $\\overrightarrow{\\mu}_{X_3}(x_3)$ can be interpreted as a **message from the red box toward variable** $x_3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We drew _directed edges_ in the FFG in order to distinguish forward messages $\\overrightarrow{\\mu}_\\bullet(\\cdot)$ (in the same direction as the arrow of the edge) from backward messages $\\overleftarrow{\\mu}_\\bullet(\\cdot)$ (in opposite direction). This is just a notational convenience since an FFG is computationally an undirected graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sum-Product Algorithm\n",
    "\n",
    "- Closing-the-box can also be interpreted as a **message update rule** for an outgoing message from a node. For a node $f(y,x_1,\\ldots,x_n)$ with incoming messages $\\overrightarrow{\\mu}_{X_1}(x_1), \\overrightarrow{\\mu}_{X_1}(x_1), \\ldots,\\overrightarrow{\\mu}_{X_n}(x_n)$, the outgoing message is given by ([Loeliger (2007), pg.1299](https://github.com/bertdv/BMLIP/blob/master/lessons/notebooks/files/Loeliger-2007-The-factor-graph-approach-to-model-based-signal-processing.pdf)): \n",
    "\n",
    "$$ \\boxed{\n",
    "\\underbrace{\\overrightarrow{\\mu}_{Y}(y)}_{\\substack{ \\text{outgoing}\\\\ \\text{message}}} = \\sum_{x_1,\\ldots,x_n} \\underbrace{\\overrightarrow{\\mu}_{X_1}(x_1)\\cdots \\overrightarrow{\\mu}_{X_n}(x_n)}_{\\substack{\\text{incoming} \\\\ \\text{messages}}} \\cdot \\underbrace{f(y,x_1,\\ldots,x_n)}_{\\substack{\\text{node}\\\\ \\text{function}}} }\n",
    "$$\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"./figures/ffg-sum-product.png\" width=\"500px\"></p>\n",
    "\n",
    "- This is called the **Sum-Product Message** (SPM) update rule. (Look at the formula to understand why it's called the SPM update rule).\n",
    "\n",
    "- Just as a final note, inference by sum-product message passing is much like replacing the sum-of-products\n",
    "$$\n",
    "ac + ad + bc + bd\n",
    "$$\n",
    "by the following product-of-sums:\n",
    "$$\n",
    "(a + b)(c + d) \\,.\n",
    "$$\n",
    "Which of these two computations is cheaper to execute?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that all SPM update rules can be computed from information that is **locally available** at each node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If the factor graph for a function $f$ has **no cycles** (i.e., the graph is a tree), then the marginal $\\bar{f}(x_3) = \\sum_{x_1,x_2,x_4,x_5,x_6,x_7}f(x_1,x_2,\\ldots,x_7)$ is given by multiplying the forward and backward messages on that edge:\n",
    "\n",
    "$$ \\boxed{\n",
    "\\bar{f}(x_3) = \\overrightarrow{\\mu}_{X_3}(x_3)\\cdot \\overleftarrow{\\mu}_{X_3}(x_3)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It follows that the marginal $\\bar{f}(x_3) = \\sum_{x_1,x_2,x_4,x_5,x_6,x_7}f(x_1,x_2,\\ldots,x_7)$ can be efficiently computed through sum-product messages. Executing inference through SP message passing is called the **Sum-Product Algorithm** (or alternatively, the **belief propagation** algorithm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"sp-for-equality-node\">Sum-Product Messages for the Equality Node</a>\n",
    "\n",
    "- As an example, let¬¥s evaluate the SP messages for the **equality node** $f_=(x,y,z) = \\delta(z-x)\\delta(z-y)$: \n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"./figures/ffg-equality-node.png\" width=\"250px\"></p>\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\overrightarrow{\\mu}_{Z}(z) &= \\iint  \\overrightarrow{\\mu}_{X}(x) \\overrightarrow{\\mu}_{Y}(y) \\,\\delta(z-x)\\delta(z-y) \\,\\mathrm{d}x \\mathrm{d}y \\\\\n",
    "   &=  \\overrightarrow{\\mu}_{X}(z)  \\int  \\overrightarrow{\\mu}_{Y}(y) \\,\\delta(z-y) \\,\\mathrm{d}y \\\\\n",
    "   &=  \\overrightarrow{\\mu}_{X}(z) \\overrightarrow{\\mu}_{Y}(z) \n",
    "\\end{align*}$$\n",
    "\n",
    "- By symmetry, this also implies (for the same equality node) that\n",
    "$$\\begin{align*}\n",
    "\\overleftarrow{\\mu}_{X}(x) &= \\overrightarrow{\\mu}_{Y}(x) \\overleftarrow{\\mu}_{Z}(x) \\quad \\text{and} \\\\\n",
    "\\overleftarrow{\\mu}_{Y}(y) &= \\overrightarrow{\\mu}_{X}(y) \\overleftarrow{\\mu}_{Z}(y)\\,.\n",
    "\\end{align*}$$\n",
    "\n",
    "- Let us now consider the case of Gaussian messages $\\overrightarrow{\\mu}_{X}(x) = \\mathcal{N}(x|\\overrightarrow{m}_X,\\overrightarrow{V}_X)$, $\\overrightarrow{\\mu}_{Y}(y) = \\mathcal{N}(y| \\overrightarrow{m}_Y,\\overrightarrow{V}_Y)$ and $\\overrightarrow{\\mu}_{Z}(z) = \\mathcal{N}(z|\\overrightarrow{m}_Z,\\overrightarrow{V}_Z)$. Let¬¥s also define the precision matrices $\\overrightarrow{W}_X \\triangleq \\overrightarrow{V}_X^{-1}$ and similarly for $Y$ and $Z$. Then applying the SP update rule leads to multiplication of two Gaussian distributions, resulting in \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\overrightarrow{W}_Z &= \\overrightarrow{W}_X + \\overrightarrow{W}_Y \\\\ \n",
    "\\overrightarrow{W}_Z \\overrightarrow{m}_z &= \\overrightarrow{W}_X \\overrightarrow{m}_X + \\overrightarrow{W}_Y \\overrightarrow{m}_Y\n",
    "\\end{align*}$$\n",
    "\n",
    "- It follows that **message passing through an equality node is similar to applying Bayes rule**, i.e., fusion of two information sources. Does this make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id=\"sp-for-addition-node\">Sum-Product Messages for the Addition Node</a>\n",
    "\n",
    "- Next, consider an **addition node** $f_+(x,y,z) = \\delta(z-x-y)$: \n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"./figures/ffg-addition-node.png\" width=\"250px\"></p>\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\overrightarrow{\\mu}_{Z}(z) &= \\iint  \\overrightarrow{\\mu}_{X}(x) \\overrightarrow{\\mu}_{Y}(y) \\,\\delta(z-x-y) \\,\\mathrm{d}x \\mathrm{d}y \\\\\n",
    "   &=  \\int  \\overrightarrow{\\mu}_{X}(x) \\overrightarrow{\\mu}_{Y}(z-x) \\,\\mathrm{d}x \\,, \n",
    "\\end{align*}$$\n",
    "\n",
    "i.e., $\\overrightarrow{\\mu}_{Z}$ is the convolution of the messages $\\overrightarrow{\\mu}_{X}$ and $\\overrightarrow{\\mu}_{Y}$.\n",
    "\n",
    "- Of course, for Gaussian messages, these update rules evaluate to\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\overrightarrow{m}_Z &= \\overrightarrow{m}_X + \\overrightarrow{m}_Y \\\\\n",
    "\\overrightarrow{V}_z &= \\overrightarrow{V}_X + \\overrightarrow{V}_Y \\,.\n",
    "\\end{align*}$$\n",
    "\n",
    "- Exercise: For the same summation node, work out the SP update rule for the backward message $\\overleftarrow{\\mu}_{X}(x)$ as a function of $\\overrightarrow{\\mu}_{Y}(y)$ and  $\\overleftarrow{\\mu}_{Z}(z)$. And further refine the answer for Gaussian messages.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Message Passing Schedules\n",
    "\n",
    "- In a tree graph, start with messages from the terminals and keep passing messages through the internal nodes towards the \"target\" variable ($x_3$ in above problem) until you have both the forward and backward message for the target variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In a tree graph, if you continue to pass messages throughout the graph, the Sum-Product Algorithm computes **exact** marginals for all hidden variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If the graph contains cycles, we have in principle an infinite tree by \"unrolling\" the graph. In this case, the SP Algorithm is not guaranteed to find exact marginals. In practice, if we apply the SP algorithm for just a few iterations (\"unrolls\"), then we often find satisfying approximate marginals.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Terminal Nodes and Processing Observations \n",
    "\n",
    "- We can use terminal nodes to represent observations, e.g., add a factor $f(y)=\\delta(y‚àí3)$ to terminate the half-edge for variable $Y$  if  $y=3$  is observed.\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"./figures/ffg-observation-y-3.png\" width=\"600px\"></p>\n",
    "\n",
    "- Terminal nodes that carry observations are denoted by small black boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "  \n",
    "- The message out of a **terminal node** (attached to only 1 edge) is the factor itself. For instance, closing a box around terminal node $f_a(x_1)$ would lead to $$\\overrightarrow{\\mu}_{X_1}(x_1) \\triangleq \\sum_{ \\stackrel{ \\textrm{enclosed} }{ \\textrm{variables} } } \\;\\prod_{\\stackrel{ \\textrm{enclosed} }{ \\textrm{factors} }} f_a(x_1) = f_a(x_1)\\,$$\n",
    "since there are no enclosed variables. \n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"./figures/ffg-terminal-node-message.png\" width=\"250px\"></p>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The message from a half-edge is $1$ (one). You can verify this by imagining that a half-edge $x$ can be terminated by a node function $f(x)=1$ without affecting any inference issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Automating Bayesian Inference by Message Passing\n",
    "\n",
    "- The foregoing message update rules can be worked out in closed-form and put into tables (e.g., see Tables 1 through 6 in [Loeliger (2007)](./files/Loeliger-2007-The-factor-graph-approach-to-model-based-signal-processing.pdf) for many standard factors such as essential probability distributions and operations such as additions, fixed-gain multiplications and branching (equality nodes).\n",
    "\n",
    "- In the optional slides below, we have worked out a few of these update rules, eg, for the [equality node](#sp-for-equality-node), the [addition node](#sp-for-addition-node) and the [multiplication node](#sp-for-multiplication-node).\n",
    "\n",
    "- If the update rules for all node types in a graph have been tabulated, then inference by message passing comes down to executing a set of table-lookup operations, thus creating a completely **automatable Bayesian inference framework**. \n",
    "\n",
    "- In our research lab [BIASlab](http://biaslab.org) (FLUX 7.060), we are developing [RxInfer.jl](https://rxinfer.ml), which is a (Julia) toolbox for automating Bayesian inference by message passing in a factor graph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Bayesian Linear Regression by Message Passing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Assume we want to estimate some function $f: \\mathbb{R}^D \\rightarrow \\mathbb{R}$ from a given data set $D = \\{(x_1,y_1), \\ldots, (x_N,y_N)\\}$, with model assumption $y_i = f(x_i) + \\epsilon_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### model specification \n",
    "\n",
    "- We will assume a linear model with white Gaussian noise and a Gaussian prior on the coefficients $w$:\n",
    "$$\\begin{align*}\n",
    "  y_i &= w^T x_i  + \\epsilon_i \\\\\n",
    "  \\epsilon_i &\\sim \\mathcal{N}(0, \\sigma^2) \\\\ \n",
    "  w &\\sim \\mathcal{N}(0,\\Sigma)\n",
    "\\end{align*}$$\n",
    "or equivalently\n",
    "$$\\begin{align*}\n",
    "p(w,\\epsilon,D) &= \\overbrace{p(w)}^{\\text{weight prior}} \\prod_{i=1}^N  \\overbrace{p(y_i\\,|\\,x_i,w,\\epsilon_i)}^{\\text{regression model}} \\overbrace{p(\\epsilon_i)}^{\\text{noise model}} \\\\\n",
    "  &= \\mathcal{N}(w\\,|\\,0,\\Sigma) \\prod_{i=1}^N \\delta(y_i - w^T x_i - \\epsilon_i) \\mathcal{N}(\\epsilon_i\\,|\\,0,\\sigma^2) \n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Inference (parameter estimation)\n",
    "\n",
    "- We are interested in inferring the posterior $p(w|D)$. We will execute inference by message passing on the FFG for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The left figure shows the factor graph for this model. \n",
    "- The right figure shows the message passing scheme. \n",
    "  \n",
    "<p style=\"text-align:center;\"><img src=\"./figures/ffg-bayesian-linear-regression.png\" width=\"500px\"></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### CODE EXAMPLE\n",
    "\n",
    "Let's solve this problem by message passing-based inference with Julia's FFG toolbox [RxInfer](https://rxinfer.ml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "using PyPlot, LinearAlgebra\n",
    "\n",
    "# Parameters\n",
    "Œ£ = 1e5 * Diagonal(I,3) # Covariance matrix of prior on w\n",
    "œÉ2 = 2.0                # Noise variance\n",
    "\n",
    "# Generate data set\n",
    "w = [1.0; 2.0; 0.25]\n",
    "N = 30\n",
    "z = 10.0*rand(N)\n",
    "x_train = [[1.0; z; z^2] for z in z] # Feature vector x = [1.0; z; z^2]\n",
    "f(x) = (w'*x)[1]\n",
    "y_train = map(f, x_train) + sqrt(œÉ2)*randn(N) # y[i] = w' * x[i] + œµ\n",
    "scatter(z, y_train); xlabel(L\"z\"); ylabel(L\"f([1.0, z, z^2]) + \\epsilon\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now build the factor graph in RxInfer, perform sum-product message passing and plot results (mean of posterior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior distribution of w: MvNormalWeightedMeanPrecision(\n",
      "xi: [343.83983856380024, 2558.2477638240034, 20631.16787603821]\n",
      "Œõ: [15.000010000030002 86.80123822140791 621.1697269203743; 86.80123822140791 621.1697369204044 4871.13879245431; 621.1697269203743 4871.13879245431 40278.988174007995]\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "using RxInfer, Random, Distributions\n",
    "@model function linear_regression(N, Œ£, œÉ2)\n",
    "\n",
    "    w ~ MvNormalMeanCovariance(constvar(zeros(3)),constvar(Œ£))\n",
    "   \n",
    "    x = datavar(Vector{Float64}, N)\n",
    "    y = datavar(Float64, N)\n",
    "    \n",
    "    for i in 1:N\n",
    "        y[i] ~ NormalMeanVariance(dot(w , x[i]), œÉ2)\n",
    "    end\n",
    "    return w, x, y\n",
    "end\n",
    "# Run message passing algorithm \n",
    "results = inference(\n",
    "    model = linear_regression(length(x_train), Œ£, œÉ2 ),\n",
    "    data  = (y = y_train, x = x_train),\n",
    "    returnvars   = (w = KeepLast()),\n",
    "    iterations   = 20\n",
    ");\n",
    "# Plot result\n",
    "w = results.posteriors[:w]\n",
    "println(\"Posterior distribution of w: $(w)\")\n",
    "scatter(z, y_train); xlabel(L\"z\"); ylabel(L\"f([1.0, z, z^2]) + œµ\");\n",
    "z_test = collect(0:0.2:12)\n",
    "x_test = [[1.0; z; z^2] for z in z_test]\n",
    "for i=1:10\n",
    "    w_sample = rand(results.posteriors[:w])\n",
    "    f_est(x) = (w_sample'*x)[1]\n",
    "    plot(z_test, map(f_est, x_test), \"k-\", alpha=0.3);\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final thoughts: Modularity and Abstraction \n",
    "\n",
    "- The great Michael Jordan (no, not [this one](https://youtu.be/cuLprHh_BRg), but [this one](https://people.eecs.berkeley.edu/~jordan/)), wrote:   \n",
    "\n",
    "  > I basically know of two principles for treating complicated systems in simple ways: the first is the principle of **modularity** and the second is the principle of **abstraction**. I am an apologist for computational probability in machine learning because I believe that probability theory implements these two principles in deep and intriguing ways ‚Äî namely through factorization and through averaging. Exploiting these two mechanisms as fully as possible seems to me to be the way forward in machine learning. ‚Äî Michael Jordan, 1997 (quoted in [Fre98](https://mitpress.mit.edu/9780262062022/)).\n",
    "\n",
    "- Factor graphs realize these ideas nicely, both visually and computationally.\n",
    "\n",
    "- Visually, the modularity of conditional independencies in the model are displayed by the graph structure. Each node hides internal complexity and by closing-the-box, we can hierarchically move on to higher levels of abstraction. \n",
    "\n",
    "- Computationally, message passing-based inference uses the Distributive Law to avoid any unnecessary computations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>OPTIONAL SLIDES</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  <a id=\"sp-for-multiplication-node\">Sum-Product Messages for Multiplication Nodes</a>\n",
    "- Next, let us consider a **multiplication** by a fixed (invertible matrix) gain $f_A(x,y) = \\delta(y-Ax)$\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"./figures/ffg-gain-node.png\" width=\"250px\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\begin{align*}\n",
    "\\overrightarrow{\\mu}_{Y}(y) = \\int  \\overrightarrow{\\mu}_{X}(x) \\,\\delta(y-Ax) \\,\\mathrm{d}x = \\overrightarrow{\\mu}_{X}(A^{-1}y) \\,.\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For a Gaussian message input message $\\overrightarrow{\\mu}_{X}(x) = \\mathcal{N}(x|\\overrightarrow{m}_{X},\\overrightarrow{V}_{X})$, the output message is also Gaussian with \n",
    "$$\\begin{align*}\n",
    "\\overrightarrow{m}_{Y} = A\\overrightarrow{m}_{X} \\,,\\,\\text{and}\\,\\,\n",
    "\\overrightarrow{V}_{Y} = A\\overrightarrow{V}_{X}A^T\n",
    "\\end{align*}$$\n",
    "since \n",
    "$$\\begin{align*}\n",
    "\\overrightarrow{\\mu}_{Y}(y) &= \\overrightarrow{\\mu}_{X}(A^{-1}y) \\\\\n",
    "  &\\propto \\exp \\left( -\\frac{1}{2} \\left( A^{-1}y - \\overrightarrow{m}_{X}\\right)^T \\overrightarrow{V}_{X}^{-1} \\left(  A^{-1}y - \\overrightarrow{m}_{X}\\right)\\right) \\\\\n",
    "   &= \\exp \\big( -\\frac{1}{2} \\left( y - A\\overrightarrow{m}_{X}\\right)^T \\underbrace{A^{-T}\\overrightarrow{V}_{X}^{-1} A^{-1}}_{(A \\overrightarrow{V}_{X} A^T)^{-1}} \\left( y - A\\overrightarrow{m}_{X}\\right)\\big) \\\\\n",
    "  &\\propto  \\mathcal{N}(y| A\\overrightarrow{m}_{X},A\\overrightarrow{V}_{X}A^T) \\,.\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- <div class=\"exercise\"><b>Excercise</b>: Proof that, for the same factor $\\delta(y-Ax)$ and Gaussian messages, the (backward) sum-product message $\\overleftarrow{\\mu}_{X}$ is given by \n",
    "$$\\begin{align*}\n",
    "\\overleftarrow{\\xi}_{X} &= A^T\\overleftarrow{\\xi}_{Y} \\\\\n",
    "\\overleftarrow{W}_{X} &= A^T\\overleftarrow{W}_{Y}A\n",
    "\\end{align*}$$\n",
    "where $\\overleftarrow{\\xi}_X \\triangleq \\overleftarrow{W}_X \\overleftarrow{m}_X$ and $\\overleftarrow{W}_{X} \\triangleq \\overleftarrow{V}_{X}^{-1}$ (and similarly for $Y$).</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Code example: Gaussian forward and backward messages for the Addition node\n",
    "\n",
    "Let's calculate the Gaussian forward and backward messages for the addition node in RxInfer. \n",
    "<p style=\"text-align:center;\"><img src=\"./figures/ffg-addition-node.png\" width=\"250px\"></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward message on Z:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NormalMeanVariance{Float64}(Œº=3.0, v=2.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "println(\"Forward message on Z:\")\n",
    "RxInfer.@call_rule typeof(+)(:out, Marginalisation) (m_in1 = NormalMeanVariance(1.0, 1.0), m_in2 = NormalMeanVariance(2.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward message on X:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NormalMeanVariance{Float64}(Œº=1.0, v=2.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "println(\"Backward message on X:\")\n",
    "RxInfer.@call_rule typeof(+)(:in1, Marginalisation) (m_out = NormalMeanVariance(3.0, 1.0), m_in2 = NormalMeanVariance(2.0, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Code Example: forward and backward messages for the Matrix Multiplication node\n",
    "\n",
    "In the same way we can also investigate the forward and backward messages for the matrix multiplication (\"gain\") node \n",
    "<p style=\"text-align:center;\"><img src=\"./figures/ffg-gain-node.png\" width=\"250px\"></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward message on Y:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NormalMeanVariance{Float64}(Œº=4.0, v=16.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "println(\"Forward message on Y:\")\n",
    "@call_rule typeof(*)(:out, Marginalisation) (m_A = PointMass(4.0), m_in = NormalMeanVariance(1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward message on X:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NormalWeightedMeanPrecision{Float64}(xi=8.0, w=16.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "println(\"Backward message on X:\")\n",
    "@call_rule typeof(*)(:in, Marginalisation) (m_out = NormalMeanVariance(2.0, 1.0), m_A = PointMass(4.0), meta = TinyCorrection())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " \n",
    " ### Example: Sum-Product Algorithm to infer a posterior\n",
    " \n",
    " - Consider a generative model \n",
    "$$p(x,y_1,y_2) = p(x)\\,p(y_1|x)\\,p(y_2|x) .$$ \n",
    "  - This model expresses the assumption that $Y_1$ and $Y_2$ are independent measurements of $X$.\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"./figures/ffg-observations.png\" width=\"375px\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " \n",
    "- Assume that we are interested in the posterior for $X$ after observing $Y_1= \\hat y_1$ and $Y_2= \\hat y_2$. The posterior for $X$ can be inferred by applying the sum-product algorithm to the following graph:\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"./figures/ffg-observations-2.png\" width=\"650px\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - (Note that) we usually draw terminal nodes for observed variables in the graph by smaller solid-black squares. This is just to help the visualization of the graph, since the computational rules are no different than for other nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Code for Sum-Product Algorithm to infer  a posterior\n",
    "\n",
    "We'll use RxInfer to build the above graph, and perform sum-product message passing to infer the posterior $p(x|y_1,y_2)$. We assume $p(y_1|x)$ and $p(y_2|x)$ to be Gaussian likelihoods with known variances:\n",
    "$$\\begin{align*}\n",
    "    p(y_1\\,|\\,x) &= \\mathcal{N}(y_1\\,|\\,x, v_{y1}) \\\\\n",
    "    p(y_2\\,|\\,x) &= \\mathcal{N}(y_2\\,|\\,x, v_{y2})\n",
    "\\end{align*}$$\n",
    "Under this model, the posterior is given by:\n",
    "$$\\begin{align*}\n",
    "    p(x\\,|\\,y_1,y_2) &\\propto \\overbrace{p(y_1\\,|\\,x)\\,p(y_2\\,|\\,x)}^{\\text{likelihood}}\\,\\overbrace{p(x)}^{\\text{prior}} \\\\\n",
    "    &=\\mathcal{N}(x\\,|\\,\\hat{y}_1, v_{y1})\\, \\mathcal{N}(x\\,|\\,\\hat{y}_2, v_{y2}) \\, \\mathcal{N}(x\\,|\\,m_x, v_x) \n",
    "\\end{align*}$$\n",
    "so we can validate the answer by solving the Gaussian multiplication manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum-product message passing result: p(x|y1,y2) = ùí©(1.1428571428571428,0.5714285714285714)\n",
      "Manual result: p(x|y1,y2) = ùí©(1.1428571428571428, 0.5714285714285714)\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "y1_hat = 1.0\n",
    "y2_hat = 2.0\n",
    "\n",
    "# Construct the factor graph\n",
    "@model function my_model()\n",
    "\n",
    "    # `x` is the hidden states\n",
    "    x = randomvar()\n",
    "    # `y1` and `y2` are \"clamped\" observations\n",
    "    y1 = datavar(Float64,)\n",
    "    y2 = datavar(Float64,)\n",
    "    \n",
    "    x ~ NormalMeanVariance(constvar(0.0), constvar(4.0))\n",
    "    y1 ~ NormalMeanVariance(x, constvar(1))\n",
    "    y2 ~ NormalMeanVariance(x, constvar(2))\n",
    "    \n",
    "    return x\n",
    "end\n",
    "\n",
    "result = inference(model=my_model(), data=(y1=y1_hat, y2 = y2_hat,))\n",
    "println(\"Sum-product message passing result: p(x|y1,y2) = ùí©($(mean(result.posteriors[:x])),$(var(result.posteriors[:x])))\")\n",
    "\n",
    "# Calculate mean and variance of p(x|y1,y2) manually by multiplying 3 Gaussians (see lesson 4 for details)\n",
    "v = 1 / (1/4 + 1/1 + 1/2)\n",
    "m = v * (0/4 + y1_hat/1.0 + y2_hat/2.0)\n",
    "println(\"Manual result: p(x|y1,y2) = ùí©($(m), $(v))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!--\n",
       "This HTML file contains custom styles and some javascript.\n",
       "Include it a Jupyter notebook for improved rendering.\n",
       "-->\n",
       "\n",
       "<!-- Fonts -->\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Arvo:400,700,400italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=PT+Mono' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Shadows+Into+Light' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Nixie+One' rel='stylesheet' type='text/css'>\n",
       "\n",
       "<!-- Custom style -->\n",
       "<style>\n",
       "\n",
       "@font-face {\n",
       "    font-family: \"Computer Modern\";\n",
       "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "}\n",
       "\n",
       "#notebook_panel { /* main background */\n",
       "    background: rgb(245,245,245);\n",
       "}\n",
       "\n",
       "div.container {\n",
       "    min-width: 960px;\n",
       "}\n",
       "\n",
       "div #notebook { /* centre the content */\n",
       "    background: #fff; /* white background for content */\n",
       "    margin: auto;\n",
       "    padding-left: 0em;\n",
       "}\n",
       "\n",
       "#notebook li { /* More space between bullet points */\n",
       "    margin-top:0.8em;\n",
       "}\n",
       "\n",
       "/* draw border around running cells */\n",
       "div.cell.border-box-sizing.code_cell.running {\n",
       "    border: 1px solid #111;\n",
       "}\n",
       "\n",
       "/* Put a solid color box around each cell and its output, visually linking them*/\n",
       "div.cell.code_cell {\n",
       "    background-color: rgb(256,256,256);\n",
       "    border-radius: 0px;\n",
       "    padding: 0.5em;\n",
       "    margin-left:1em;\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render{\n",
       "    font-family: 'Alegreya Sans' sans-serif;\n",
       "    line-height: 140%;\n",
       "    font-size: 125%;\n",
       "    font-weight: 400;\n",
       "    width:800px;\n",
       "    margin-left:auto;\n",
       "    margin-right:auto;\n",
       "}\n",
       "\n",
       "\n",
       "/* Formatting for header cells */\n",
       ".text_cell_render h1 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-style:regular;\n",
       "    font-weight: 400;\n",
       "    font-size: 45pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-top: 0.5em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h2 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 30pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.1em;\n",
       "    margin-top: 0.3em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h3 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    margin-top:16px;\n",
       "    font-size: 22pt;\n",
       "    font-weight: 600;\n",
       "    margin-bottom: 3px;\n",
       "    font-style: regular;\n",
       "    color: rgb(102,102,0);\n",
       "}\n",
       "\n",
       ".text_cell_render h4 {    /*Use this for captions*/\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-size: 14pt;\n",
       "    text-align: center;\n",
       "    margin-top: 0em;\n",
       "    margin-bottom: 2em;\n",
       "    font-style: regular;\n",
       "}\n",
       "\n",
       ".text_cell_render h5 {  /*Use this for small titles*/\n",
       "    font-family: 'Nixie One', sans-serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 16pt;\n",
       "    color: rgb(163,0,0);\n",
       "    font-style: italic;\n",
       "    margin-bottom: .1em;\n",
       "    margin-top: 0.8em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h6 { /*use this for copyright note*/\n",
       "    font-family: 'PT Mono', sans-serif;\n",
       "    font-weight: 300;\n",
       "    font-size: 9pt;\n",
       "    line-height: 100%;\n",
       "    color: grey;\n",
       "    margin-bottom: 1px;\n",
       "    margin-top: 1px;\n",
       "}\n",
       "\n",
       ".CodeMirror{\n",
       "    font-family: \"PT Mono\";\n",
       "    font-size: 90%;\n",
       "}\n",
       "\n",
       ".boxed { /* draw a border around a piece of text */\n",
       "  border: 1px solid blue ;\n",
       "}\n",
       "\n",
       "h4#CODE-EXAMPLE,\n",
       "h4#END-OF-CODE-EXAMPLE {\n",
       "    margin: 10px 0;\n",
       "    padding: 10px;\n",
       "    background-color: #d0f9ca !important;\n",
       "    border-top: #849f81 1px solid;\n",
       "    border-bottom: #849f81 1px solid;\n",
       "}\n",
       "\n",
       ".emphasis {\n",
       "    color: red;\n",
       "}\n",
       "\n",
       ".exercise {\n",
       "    color: green;\n",
       "}\n",
       "\n",
       ".proof {\n",
       "    color: blue;\n",
       "}\n",
       "\n",
       "code {\n",
       "  padding: 2px 4px !important;\n",
       "  font-size: 90% !important;\n",
       "  color: #222 !important;\n",
       "  background-color: #efefef !important;\n",
       "  border-radius: 2px !important;\n",
       "}\n",
       "\n",
       "/* This removes the actual style cells from the notebooks, but no in print mode\n",
       "   as they will be removed through some other method */\n",
       "@media not print {\n",
       "  .cell:nth-last-child(-n+2) {\n",
       "    display: none;\n",
       "  }\n",
       "}\n",
       "\n",
       "footer.hidden-print {\n",
       "    display: none !important;\n",
       "}\n",
       "    \n",
       "</style>\n",
       "\n",
       "<!-- MathJax styling -->\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        CommonHTML: {\n",
       "                            scale: 200\n",
       "                        },\n",
       "                TeX: {\n",
       "                    extensions: [\"AMSmath.js\"],\n",
       "                    equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\n",
       "                },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "open(\"../../../styles/aipstyle.html\") do f display(\"text/html\", read(f, String)) end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.9.2",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
